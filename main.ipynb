{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "B1KBh-7VSW5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9X0WSX_8bLM"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "RE0ZFe07UtSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "df = pd.read_csv('yelp.csv')"
      ],
      "metadata": {
        "id": "pZZuMksu8rRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Preprocessing (Removing punctuation marks and other characters and making the text data lowercase)\n",
        "import re\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "df_filtered = df_filtered.copy()\n",
        "df_filtered['Review'] = df_filtered['Review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "0EF6KerzM_Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Vectorizing (Converting the text data into numerical values)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df_filtered['Review']).toarray()"
      ],
      "metadata": {
        "id": "dSEBe-RCNg5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Test Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df_filtered['Label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "YvZ7RB2WOTU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "IyrvxWesOTh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Visualization**"
      ],
      "metadata": {
        "id": "H95wMC4bU7Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Z-9xH1iga6Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pie Chart : To visualize the ratings distribution of each real and fake reviews\n",
        "# For Real Reviews\n",
        "df_positive_label = df[df[\"Label\"] == 1]\n",
        "\n",
        "# Count the frequency of each rating having real label\n",
        "rating_counts = df_positive_label[\"Rating\"].value_counts()\n",
        "\n",
        "# Create a pie chart\n",
        "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title(\"Frequency of Ratings for Label = 1\")\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# For Fake Reviews\n",
        "df_negative_label = df[df[\"Label\"] == -1]\n",
        "\n",
        "# Count the frequency of each rating having fake label\n",
        "rating_counts = df_negative_label[\"Rating\"].value_counts()\n",
        "\n",
        "# Create a pie chart\n",
        "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title(\"Frequency of Ratings for Label = -1\")\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4nfuGrY_XvDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot : To visualize the length of reviews and compare them as real and fake reviews\n",
        "df['Review_Length'] = df['Review'].apply(len)\n",
        "\n",
        "# Plotting the data points\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Label', y='Review_Length', data=df, hue='Label', palette='pastel', dodge=True)\n",
        "plt.title('Review Length Distribution by Label')\n",
        "plt.xlabel('Label (1: Real, -1: Fake)')\n",
        "plt.ylabel('Review Length')\n",
        "plt.legend(title=None)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8YsIru0IYt5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Line Graph / Time Series Graph : To visualize the number of reviews given over the time both real and fake (Date Vs No of Reviews)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Plotting the datapoints\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='Year', y='count', hue='Label', data=df.groupby(['Year', 'Label']).size().reset_index(name='count'), marker='o')\n",
        "plt.title('Temporal Trends in Number of Reviews by Label')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hZoKyJEdZDda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Graph : To visualize the 10 most frequent words present in all fake reviews and giving their frequency\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "X = vectorizer.fit_transform(df['Review'])\n",
        "\n",
        "# Create a DataFrame with word frequencies\n",
        "word_freq_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add the 'Label' column to the DataFrame\n",
        "word_freq_df['Label'] = df['Label']\n",
        "\n",
        "# Calculate the average frequency of each word for real and fake reviews\n",
        "average_word_freq = word_freq_df.groupby('Label').mean().transpose()\n",
        "\n",
        "# Choose the top N words\n",
        "top_words = average_word_freq.sort_values(by=-1).head(10)  # Replace 10 with the desired number of words\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=top_words.reset_index(), x='index', y=-1)\n",
        "plt.title('Top Words in Fake Reviews')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Average Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MMTUA5lxbWxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud\n",
        "from wordcloud import WordCloud\n",
        "reviews_text = ' '.join(df['Review'].astype(str))\n",
        "\n",
        "# Generate Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, random_state=42, background_color='white').generate(reviews_text)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Reviews')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lFUDJztxaV3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap (Comparing the relations among User_Id, Product_Id, Rating, Label)\n",
        "# Calculate the correlation among columns matrix\n",
        "corr_matrix = df_sample[['User_id', 'Product_id', 'Rating', 'Label']].corr()\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
        "plt.title('Heatmap: Correlation between User_Id, Product_Id, Rating, and Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uNuaaQQeLzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Chart (Applying sentiment analysis and comparing the real and fake reviews)\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis and classify each review\n",
        "df_sample['Sentiment'] = df_sample['Review'].apply(lambda x: 'Positive' if sia.polarity_scores(x)['compound'] >= 0.5 else ('Negative' if sia.polarity_scores(x)['compound'] <= -0.5 else 'Neutral'))\n",
        "\n",
        "# Create a grouped bar chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.countplot(x='Sentiment', hue='Label', data=df_sample, palette={1: 'blue', -1: 'red'})\n",
        "plt.title('Sentiment Analysis: Polarized Sentiment Distribution for Real and Fake Reviews')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "otCcxnX3L0B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre Model Building Tasks**"
      ],
      "metadata": {
        "id": "UnM0fgnWPAcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifying the Dataset based on the insights from visualizations\n",
        "# [Note : Perform these tasks before splitting the data in train and test]\n",
        "\n",
        "# 1. Using the 4 & 5 stars rating from Rating column, Taking the reviews from the year 2014\n",
        "df_filtered = df_sampled[(df_sampled['Rating'].isin([4, 5])) & (df_sampled['Date'].str.contains('2014'))]\n",
        "\n",
        "# 2. Dropping the User_id and Product_id columns\n",
        "df_filtered.drop(columns=['User_id', 'Product_id'], inplace=True)\n",
        "\n",
        "# 3. Using Sentiment Analysis to filter out positive reviews for the model\n",
        "from textblob import TextBlob\n",
        "def calculate_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    # Assign sentiment labels based on polarity (adjust threshold as needed)\n",
        "    return 'Positive' if analysis.sentiment.polarity > 0 else 'Negative'\n",
        "\n",
        "df_filtered = df_filtered.copy()\n",
        "# Apply sentiment analysis to the 'Review' column\n",
        "df_filtered['Sentiment'] = df_filtered['Review'].apply(calculate_sentiment)\n",
        "\n",
        "# Filter out positive reviews\n",
        "df_filtered = df_filtered[df_filtered['Sentiment'] == 'Positive']\n",
        "df_filtered.drop(columns=['Sentiment'], inplace=True)"
      ],
      "metadata": {
        "id": "aTEJGbTwPN8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building, Training, Testing and Evaluating**"
      ],
      "metadata": {
        "id": "7I0m9O5dRQTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Building and Training\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "\n",
        "# Showing the Accuracy and F1 Score of the Model\n",
        "print(\"SVM Model:\")\n",
        "print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"F1 Score: {f1_svm:.4f}\")"
      ],
      "metadata": {
        "id": "FG5T1gZ5RZZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Building and Training\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "# Showing the Accuracy and F1 Score of the Model\n",
        "print(\"\\nRandom Forest Model:\")\n",
        "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"F1 Score: {f1_rf:.4f}\")"
      ],
      "metadata": {
        "id": "IkHqFRyQRqWP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}